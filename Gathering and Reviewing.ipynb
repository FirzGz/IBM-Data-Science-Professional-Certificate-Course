{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target:\n",
    " - Skills:\n",
    "  - Regression\n",
    "  - Classification\n",
    "  - Clustering\n",
    "  - Scikit Learn\n",
    "  - Scipy\n",
    "  - Supervised Learning and Unsupervised Learning\n",
    " \n",
    " \n",
    "Machine Learning is the subfield of computer science that gives ?computers the ability to learn without being explicitly programmed.\" - Arthur Samuel\n",
    "\n",
    "Major machine learning techniques:\n",
    " - Regression/Estimation: Predicting continuous values\n",
    "     -> Ex: * Predicting things like price of a house base on its characteristics\n",
    "            * Estimate CO2 emssion from a car's engine\n",
    " - Classification: Predicting the item class/category of a case\n",
    "     -> Ex: * If a cell is benign or malignant, or whether or not a customer will churn\n",
    " - Clustering: Finding the structure of data; summarization\n",
    "     -> Ex: * Clustering groups of similar cases like find similar patients\n",
    "            * Can be used for customer segmentation in the banking field\n",
    " - Associations: Associating frequent co-occuring items/events\n",
    "     -> Ex: * Grocery items that are usually bought together by a particular customer\n",
    " - Anomaly detection: Discovering abnormal and unusual cases\n",
    "     -> Ex: * Used for credit card fraud detection\n",
    " - Sequence mining: Predicting next events; click-stream in website (Markov Model, HMM)\n",
    " - Dimension Reduction: Reducing the size of data (PCA)\n",
    " - Recommendation systems: Recommending items\n",
    "     -> Ex: Associates peooples' preferences with others who have similar tastes, and recommends new items to them, such as books or movies\n",
    "\n",
    "\n",
    "Difference between artificial intelligence, machine learning, and deep learning\n",
    " - AI Components: Computer Vision, Language Processing, Creativity, Summarization, etc. -> make computers intelligent in order to mimic the cognitive functions of human\n",
    " - Machine Learning: Classification, Clustering, Neural Network, etc. -> branch of AI that covers the statistical part of artificial intelligent, teaches the computer to soolve problems by looking at hundreds or thousands of examples, learning from them, and then using that experience to solve the same problem in new situations\n",
    " - Revolution in ML: Deep Learning -> computer can actually learn and make intelligent decisions on their own, Deep learning involves a deeper level of automation in comparison with most machine learning algorthims\n",
    " \n",
    "#### Review:\n",
    " - Supervised Learning: \n",
    "     - Is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. \n",
    "     - It infers a function from labeled training data consiting of a set of training examples, can be used for mapping new examples\n",
    "     - Most widely used learning algorithms:\n",
    "         - Support Vector Machines (SVM) for Classification\n",
    "         - Linear Regression\n",
    "         - Logistic Regression\n",
    "         - Naive Bayes\n",
    "         - Linear Discriminant Analysis\n",
    "         - Decision Trees\n",
    "         - K-nearest neighbor\n",
    "         - Neural Networks (Multilayer perceptron)\n",
    "     \n",
    " - Unsupervised Learning:\n",
    "     - Is where only have input data X and no corresonding output variables\n",
    "     - The goal is to model the underlying structure or distribution in the data in order to learn more about the data\n",
    "     - Most Widely used learning algorithms:\n",
    "         - Clustering\n",
    "         - Association\n",
    "         - K-means\n",
    "         - Apriori\n",
    "         - Dimension reduction\n",
    "         - Density Estimation\n",
    "         - Market basket analysis\n",
    "         \n",
    "![picture_alt](https://image.ibb.co/f4xJ49/image.png)\n",
    "         \n",
    " - Semi-Supervised Learning:\n",
    "     - Is where having input data X and some of the data is labeled Y\n",
    "     - Problems sit between both supervised and unsupervised learning\n",
    "     - Can use supervised learning techniques to make best predictions for the unlabled data, feed that data back into the supervised learning algorithm as training data and use the model to make predictions on new unseen data\n",
    "     - Can use unsupervised learning techniques to discover and learn the structure in the input variables\n",
    "\n",
    "#### Python libraries for machine learning\n",
    " - NumPy: a math library to work with N-Dimensional arrays, enable to do computation efficiently and effectively (Ex usage: arrays, dictionaries, functions, datatypes)\n",
    " - SciPy: a collection of numerical algorithms and domain specific toolboxes, including signal processing, optimization, statistics, etc., high performance computation\n",
    " - Matplotlib: provides 2D plotting, awa 3D plotting\n",
    " - Pandas: high level library provides high performance, easy to use data structures, has many functions for data importing, manipulation and analysis, offers data structures and operations for manipulating numerical tables and timeseries\n",
    " - __Scikit Learn__: a collection of algorithm and tools for machine learning, has most with the Classification, Regression and Clustering algorithms, designed to work with Python numerical and scientific libraries: NumPy and SciPy, includes very good documentation, less Python code. Most of the tasks need to be done in a machine learning pipeline are implemented already in Scikit Learn including pre-processing of data, feature selection, feature extraction, train test splitting, defining the algorithms, fitting models, tuning parameters, prediction, evaluation and exporting the model\n",
    "![picture_alt](https://image.ibb.co/fntDMp/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    " - Simple Linear Regression\n",
    " - Multiple Linear Regression\n",
    "\n",
    "#### Review Regression Metrics\n",
    " - Mean Squared Error (MSE)\n",
    " - Root Mean Squared Error (RMSE)\n",
    " - Mean Absolute Error (MAE)\n",
    " - R Square ($R^2$)\n",
    " - Mean Square Percentage Error (MSPE)\n",
    " - Mean Absolute Percentage Error (MAPE)\n",
    " - Root Mean Squared Logarithmic Error (RMSLE)\n",
    " \n",
    "###### Mean Squared Error (MSE): \n",
    "\\begin{equation*}\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2  \n",
    "\\end{equation*}\n",
    "As $y_i$ is the actual expected output and $\\hat{y}_i$ is the models prediction\n",
    " - Most simple and common metric for regression evaluation, least useful\n",
    " - Basically measures average squared error of our predictions. For each point, it calculates square difference between the predictions and the target and then average those values\n",
    " - The higher this value, the worse the model is. It would be zero for a perfect model\n",
    " - Pros: \n",
    "     - Useful if we have unexpected values that we should care about\n",
    "     - Very high or low value that we should pay attention\n",
    " - Cons:\n",
    "     - If we make a single very bad prediction, the squaring will make the error even worse and it may skew the metric towards overestimating the model's badness. That is a particularly problematic behaviour if we have noisy data (that is, data that for whatever reason is not entirely reliable)\n",
    "     - Even a “perfect” model may have a high MSE in that situation, so it becomes hard to judge how well the model is performing. On the other hand, if all the errors are small, or rather, smaller than 1, than the opposite effect is felt: we may underestimate the model’s badness.\n",
    " - Note: if we want to have a constant prediction the best one will be the __mean value of the target values__. It can be found by setting the derivative of our total error with respect to that constant to zero, and find it from this equation.\n",
    "\n",
    "###### Root Mean Squared Error (RMSE)\n",
    "\\begin{equation*}\n",
    "RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2} = \\sqrt{MSE} \n",
    "\\end{equation*}\n",
    " - The square root is introduced to make scale of the errors to be the same as the scale of the targets\n",
    " - $MSE$ and $RMSE$ are similar in terms of their minimizers, every minimizer of MSE is also a minimizer for $RMSE$ and vice versa since the square root is an non-decreasing function. For example, if we have two sets of predictions, $A$ and $B$, and say $MSE$ of $A$ is greater than MSE of $B$, then we can be sure that $RMSE$ of $A$ is greater $RMSE$ of $B$. And it also works in the opposite direction.  \n",
    " \\begin{equation*}\n",
    " MSE(a) > MSE(b) \\Leftrightarrow RMSE(a) > RMSE(b)\n",
    " \\end{equation*}  \n",
    " - It means that if the target metric is RMSE, we still can compare our models using MSE, since MSE will order the models in the same way as RMSE. Thus we can optimize MSE instead of RMSE.\n",
    " - MSE is a little bit easier to work with, so everybody uses MSE instead of RMSE. Also a little bit of difference between the two for gradient-based models.  \n",
    " \\begin{equation*}\n",
    " \\frac{\\partial RMSE}{ \\partial \\hat{y}_i} = \\frac{1}{2 \\sqrt{MSE}} \\frac{\\partial MSE}{\\partial \\hat{y}_i}  -  \n",
    " \\text{Gradient of RMSE with respect to i-th prediction}\n",
    " \\end{equation*}  \n",
    " - It means that travelling along MSE gradient is equivalent to traveling along RMSE gradient but with a diferent flowing rate and the flowing rate depends on MSE score itself.\n",
    " - So even though RMSE and MSE are really similar in terms of models scoring, the can be not immediately interchangeable for gradient based methods. We will probably need to adjust some parameters like the learning rate.\n",
    " \n",
    "###### Mean Absolute Error (MAE)\n",
    " - In MAE the error is calculated as an average of absolute differences between the target values and the predictions. The MAE is a linear score which means that all the individual differences are weighted equally in the average. For example, the difference between 10 and 0 will be twice the difference between 5 and 0. However, same is not true for RMSE. Mathematically, it is calculated using this formula:\n",
    " \\begin{equation*}\n",
    " MAE = \\frac{1}{N} \\sum_{i=1}^N \\vert y_i - \\hat{y}_i \\vert\n",
    " \\end{equation*} \n",
    " - What is important about this metric is that it penalizes huge errors that not as that badly as MSE does. Thus, it’s not that sensitive to outliers as mean square error.\n",
    " - MAE is widely used in finance, where $\\$$10 error is usually exactly two times worse than $\\$$5 error. On the other hand, MSE metric thinks that $\\$$10 error is four times worse than $\\$$5 error. MAE is easier to justify than RMSE.\n",
    " - Another important thing about MAE is its gradients with respect to the predictions.The gradiend is a step function and it takes $-1$ when $\\hat{y}$ is smaller than the target and $+1$ when it is larger.\n",
    " - Now, the gradient is not defined when the prediction is perfect,because when $\\hat{y}$ is equal to $y$, we can not evaluate gradient. It is not defined.\n",
    " ![picture_alt](https://image.ibb.co/gr586L/image.png)\n",
    " - So formally, MAE is not differentiable, but in fact, how often your predictions perfectly measure the target. Even if they do, we can write a simple IF condition and returnzero when it is the case and through gradient otherwise. Also know that second derivative is zero everywhere and not defined in the point zero.\n",
    " - Note that if we want to have a constant prediction the best one will be the median value of the target values. It can be found by setting the derivative of our total error with respect to that constant to zero, and find it from this equation.\n",
    " \n",
    "###### R Square (R^2)\n",
    " - Now, what if I told you that MSE for my models predictions is $32$? Should I improve my model or is it good enough?Or what if my MSE was $0.4$?Actually, it’s hard to realize if our model is good or not by looking at the absolute values of MSE or RMSE.We would probably want to measure how much our model is better than the constant baseline.\n",
    "\n",
    " - The coefficient of determination, or R² (sometimes read as R-two), is another metric we may use to evaluate a model and it is closely related to MSE, but has the advantage of being scale-free — it doesn’t matter if the output values are very large or very small, the R² is always going to be between $-\\infty$ and $1$.\n",
    "\n",
    " - When $R^2$ is negative it means that the model is worse than predicting the mean.\n",
    " \\begin{equation*}\n",
    "R^2 = 1 - \\frac{MSE(model)}{MSE(baseline)}\n",
    "\\end{equation*}\n",
    " - The MSE of the model is computed as above, while the MSE of the baseline is defined as:\n",
    " \\begin{equation*}\n",
    " MSE(baseline) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    " \\text{Where the y with a bar is the mean of the observed y_i}\n",
    " \\end{equation*}  \n",
    " - To make it more clear, this baseline MSE can be thought of as the MSE that the simplest possible model would get. The simplest possible model would be to always predict the average of all samples. A value close to $1$ indicates a model with close to zero error, and a value close to zero indicates a model very close to the baseline.\n",
    " - In conclusion, $R^2$ is the ratio between how good our model is vs how good is the naive mean model.\n",
    " - Common Misconception: Alot of articles in the web states that the range of $R^2$ lies between $0$ and $1$ which is not actually true. The maximum value of $R^2$ is $1$ but minimum can be $-\\infty$.\n",
    " - For example, consider a really crappy model predicting highly negative value for all the observations even though $y_{actual}$ is positive. In this case, $R^2$ will be less than $0$. This is a highly unlikely scenario but the possibility still exists.\n",
    " \n",
    "###### Mean Square Percentage Error (MSPE)\n",
    "###### Mean Absolute Percentage Error (MAPE)\n",
    "###### Root Mean Squared Logarithmic Error (RMSLE)\n",
    "\n",
    "- Tips:\n",
    "    * Have outliers in the data? $\\Rightarrow$ Use MAE\n",
    "    * Unexpected values? $\\Rightarrow$ Use MAE\n",
    "- MAE is less sensitive to outliers than MSE but it doesn't mean always better to MAE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "Variable must be ___continuos___  \n",
    "\n",
    "Continuos|Discrete\n",
    "---------|--------\n",
    "Can take on any value between 2 specified values or specified range| Otherwise, can only take certain values\n",
    "Eg.: A $\\in (10; 20)$ so A is Continuos| B $\\in (-1; +\\infty)$ so B is Discrete\n",
    "A person's height, not just certain fixed heights; Time in a race, could even measure it to fractions of a second| The number of students in a class (there is no 0.5 student)\n",
    "\n",
    "It seldom makes sense to consider categorical data like (gender: male or female) as Continuous\n",
    "\n",
    " - Simple Regression:\n",
    "     * Simple Linear Regression\n",
    "     * Simple Non-Linear Regression\n",
    " - Multiple Regression:\n",
    "     * Multiple Linear Regression\n",
    "     * Multiple Non-Linear Regression   \n",
    "     \n",
    " $\\Rightarrow$ Depend on Independent Variables and Dependent Variables, it can be either Linear or Non-Linear Regression\n",
    "   \n",
    "  Sample of Applications using Regression:\n",
    "  - Sales forecasting\n",
    "  - Satisfaction analysis\n",
    "  - Price estimation\n",
    "  - Employment income\n",
    "  \n",
    "  Regression Algorithm\n",
    "  - Ordinal regression\n",
    "  - Poisson regression\n",
    "  - Fast orest quantile regression\n",
    "  - Linear, Polynomial, Lasso, Stepwise, Ridge regression\n",
    "  - Bayesian linear regression\n",
    "  - Neural network regression\n",
    "  - Decision forest regression\n",
    "  - Boosted decision tree regression\n",
    "  - KNN (K-Nearest Neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    " - __Logistic Regression__ is a classification algorithm for categorical variables\n",
    "![picture_alt](https://image.ibb.co/e5xsBz/image.png)\n",
    " - Logistic regression is analogous to linear regression, but tries to predict a categorical or discrete target field instead of a numeric one, that mean predicting binary variable like Yes/No, True/False, or anythings can be coded as 1 or 0\n",
    " - Logistic regression dependent variables should be continuous\n",
    " - If categorical, they should be dummy or indicator coded, means that we have to transform them to some continuous value\n",
    " - Common applications:\n",
    "     - Predicting the probability of a person having a heart attack within a specified time period based on our knowledge of the person's age, sex and body mass index\n",
    "     - Predicting the chance of mortality and and an injured patient or to predict whether a patient has a given disease such as diabetes based on observed chaaracteristics of that patient such as weight, height, blood pressure and results of various blood tests and so on\n",
    "     - Predicting a customer's propensity to purchase a product or halt a subscription\n",
    "     - Predicting the probability of failure of a given process, system or product\n",
    "     - Predicting the likelihood of a homeowner defaulting on a mortgage\n",
    "\n",
    "#### When should we use logistic regression?\n",
    "![picture_alt](https://image.ibb.co/e7BG4K/image.png)\n",
    " - If your data is binary: $0/1$, $Yes/No$, $True/False$, $+/-$\n",
    " - If you need probabilistic results. Logistic Regression returns a probability score between zero and one for a given sample of data, and we map the cases to a discrete class based on that probability\n",
    " - If your data is linearly separable or when you need a linear dicision boundary. The decisioon boundary of logistic regression is a line or a plane or a hyper plane. A classifier will classify all the points on one side of the decision boundary as beloging to one class, and all those on the other side as belonging to the other class. Eg. if we have just two features and they're not applying any polynomial processing we can obtain an inequality like $\\theta_0 + \\theta_1x_1 + \\theta_2x_2 > 0$, which is a half-plane  easily plotable. _Note:_ in using logistic regression, we can also achieve a complex decision boundary using polynomial processing as well, you'get more insight decision boundaries when you understand how logistic regression works\n",
    " - If you need to understand the impact of a feature. You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters. That is, after finding the optimum parameters, a feature $x$ with the weight $\\theta_1$ close to $0$ has a smaller effect on the prediction than features with large absolute values of $\\theta_1$. Indeed, it allows us to understand the impact an independent variable has on the dependt variable while controlling other independent variables.\n",
    "![picture_alt](https://image.ibb.co/jwVb4K/image.png)\n",
    "\n",
    "#### Logistic regression vs. Linear Regression\n",
    "Basis for comparison|Linear Regression|Logistic Regression\n",
    "--------------------|-----------------|-------------------\n",
    "Basic|The data is modelled using a straight line|The probability of some obtained event is represented as a linear function of a combination of predictor variables\n",
    "Linear relationship between dependent and independent variables|Is required|Not required\n",
    "The independent variable|Could be correlated with each other (Specially in multiple linear regression)|Should not be correlated with each other (no multicollinearity exist)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
